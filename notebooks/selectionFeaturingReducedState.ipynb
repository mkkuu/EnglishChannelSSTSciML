{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "# BONUS - Extraction d'un état réduit par sélection\n",
    "---\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "> #### Pourquoi extraire un état réduit de cette manière ?\n",
    "\n",
    "Quand on construit notre état réduit à partir de la PCA/EOF, on construit des combinaisons linéaires des variables originales. Les modes (EOF1, EOF2, etc.) n'existent pas physiquement et permettent surtout de maximiser la variance expliquée.\n",
    "\n",
    "Le défaut, c'est que dans un projet où notre objectif est de capturer la dynamique thermique d'une zone géographique définie, entraîner notre modèle sur des données synthétiques peut devenir un obstacle à l'interprétabiltié causale et physique.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Dans l'optique de pouvoir comparer avec notre état réduit initial, nous allons réaliser une sélection de *features* (une seconde famille de méthode pour réduire un jeu de donnée). Ces méthodes permettent de conserver les variables à variance élevée (celle qui sont les plus explicatives) ou à variance conditionnelle.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "---\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "## Implémentation d'une méthode de sélection\n",
    "\n",
    "Nous allons implémenter une combinaison simple (SINDy + Lasso) pour réduire notre jeu de données d'anomalies désaisonnalisées par la sélection.\n",
    "\n",
    "On commence par charger notre jeu de données post analyse statistique et importons les librairies qui nous serons utiles. On *reshape* le champ de SST en une matrice 2D en \"empilant\" les dimensions spatiales (ainsi chaque localisation correspond à un élément de la colonne spatiale).\n",
    "\n",
    "Le champ SST est initialement un champ spatio-temporel : \n",
    "\n",
    "$$\n",
    "\\text{SST}(t,\\phi,\\lambda)\n",
    "$$\n",
    "\n",
    "Et comme dans notre précédente extraction, nous le reformulons en une matrice de données :\n",
    "\n",
    "$$\n",
    "X \\in \\mathbb{R}^{T\\times N}\n",
    "$$\n",
    "\n",
    "où :\n",
    "- T = nombre d'instants temporels\n",
    "- N = nombre de point spatiaux\n",
    "\n",
    "Chaque colonne correspondra ainsi à une variable physique réelle : \n",
    "\n",
    "$$\n",
    "X(t) = [x_1(t), x_2(t), \\dots, x_N(t)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "ds = xr.open_dataset(\"data/processed/sstDeseasonalizedCOPERNICUS20102019.nc\")\n",
    "\n",
    "sst = ds[\"analysed_sst\"]\n",
    "\n",
    "# Shaping from (time, lat, lon) to (time, space)\n",
    "sstStacked = sst.stack(space=(\"latitude\", \"longitude\"))\n",
    "sstStacked = sstStacked.dropna(\"space\")\n",
    "\n",
    "X = sstStacked.values  # (T, Nspace)\n",
    "dt = 1.0  # time step in days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En suite, on applique une standardisation colonne par colonne. C'est indispensable car :\n",
    "\n",
    "- PCA/EOF maximise la variance :\n",
    "$$\n",
    "\\text{max Var}(u^{\\perp}X)\n",
    "$$\n",
    "$\\to$ sans normalisation, les régions à forte variance dominent artificiellement.\n",
    "- Lasso résout (voir la partie théorie de la méthode utilisée):\n",
    "$$\n",
    "\\underset{\\beta}{\\text{min}}||y - X\\beta||²_2 + \\alpha||\\beta||_1\n",
    "$$\n",
    "$\\to$ cela correspond à une régression linéaire classique, plus un facteur dit \"pénalisant\" $\\mathcal{l}_1$ qui dépend directement de l'échelle.\n",
    "\n",
    "Par une normalisation du type :\n",
    "\n",
    "$$\n",
    "\\tilde{x}(t) = \\frac{x(t) - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "$\\to$ on garantie que la sélection repose sur la dynamique plutôt que sur l'amplitude brute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "XScaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Pourquoi réutilise-t-on la PCA ?\n",
    "\n",
    "C'est le *twist*. On applique la PCA non pas pour utiliser les modes résultant comme variables d'état de notre état réduit mais plutôt comme outil intermédiaire. Elles nous servent uniquement commme élément d'observation/comparaison pour, par la suite, identifier les variables physiques du jeu qui structurent la dynamique globale (puis à les sélectionner)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nComponents = 100\n",
    "pca = PCA(n_components=nComponents)\n",
    "\n",
    "PCs = pca.fit_transform(XScaled)       # (T, K)\n",
    "EOFs = pca.components_.T               # (Nspace, K)\n",
    "explainedVar = pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette étape, nous classons les variables physiques en fonction de leur contribution aux modes de variabilité dominants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "topKModes = 30\n",
    "importance = np.zeros(EOFs.shape[0])\n",
    "\n",
    "for k in range(topKModes):\n",
    "    importance += explainedVar[k] * np.abs(EOFs[:, k])\n",
    "\n",
    "nCandidates = 20\n",
    "candidateIndices = np.argsort(importance)[-nCandidates:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les différences finies sont utilisées comme approximation du premier ordre ; la sensibilité au bruit est prise en compte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dXdt = np.gradient(XScaled, dt, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = PCs[:, 0]\n",
    "dTargetdt = np.gradient(target, dt)\n",
    "\n",
    "XCandidates = XScaled[:, candidateIndices]\n",
    "\n",
    "lasso = Lasso(alpha=0.005, max_iter=10000)\n",
    "lasso.fit(XCandidates, dTargetdt)\n",
    "\n",
    "selectedMask = np.abs(lasso.coef_) > 1e-6\n",
    "selectedIndices = candidateIndices[selectedMask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "XReducedScaled = XScaled[:, selectedIndices]\n",
    "dXdtReduced = np.gradient(XReducedScaled, dt, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsReduced = xr.Dataset(\n",
    "    {\n",
    "        \"XReduced\": ((\"time\", \"space\"), XReducedScaled),\n",
    "        \"dXdtReduced\": ((\"time\", \"space\"), dXdtReduced),\n",
    "    },\n",
    "    coords={\n",
    "        \"time\": ds[\"time\"],\n",
    "        \"space\": selectedIndices,\n",
    "    },\n",
    "    attrs={\n",
    "        \"description\": \"Reduced physical state obtained via EOF-based ranking and sparse dynamic selection\"\n",
    "    }\n",
    ")\n",
    "\n",
    "dsReduced.to_netcdf(\"data/processed/sstReducedStateCOPERNICUS20102019.nc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Load deseasonalized SST data\n",
    "ds = xr.open_dataset(\"data/processed/sstDeseasonalizedCOPERNICUS20102019.nc\")\n",
    "\n",
    "# Extract SST data and adapt its shape\n",
    "sst = ds[\"analysed_sst\"]\n",
    "sstStacked = sst.stack(space=(\"latitude\", \"longitude\"))\n",
    "sstStacked = sstStacked.dropna(\"space\")\n",
    "\n",
    "X = sstStacked.values  # shape (Nt, Nspace)\n",
    "\n",
    "dt = 1.0  # time step in days\n",
    "\n",
    "scaler = StandardScaler()\n",
    "XScaled = scaler.fit_transform(X)\n",
    "\n",
    "nComponents = 100\n",
    "pca = PCA(n_components=nComponents)\n",
    "\n",
    "PCs = pca.fit_transform(XScaled)     # (T, K)\n",
    "EOFs = pca.components_.T               # (N, K)\n",
    "explainedVar = pca.explained_variance_ratio_\n",
    "\n",
    "topKModes = 30\n",
    "\n",
    "importance = np.zeros(EOFs.shape[0])\n",
    "\n",
    "for k in range(topKModes):\n",
    "    importance += explainedVar[k] * np.abs(EOFs[:, k])\n",
    "\n",
    "nCandidates = 20\n",
    "candidateIndices = np.argsort(importance)[-nCandidates:]\n",
    "\n",
    "dXdt = np.gradient(XScaled, dt, axis=0)\n",
    "\n",
    "# Sparse regression using Lasso\n",
    "\n",
    "targetIndex = 0  # variable physique ou composante d’intérêt\n",
    "\n",
    "XCandidates = XScaled[:, candidateIndices]\n",
    "yTarget = dXdt[:, targetIndex]\n",
    "\n",
    "lasso = Lasso(alpha=0.005)\n",
    "lasso.fit(XCandidates, yTarget)\n",
    "\n",
    "selectedMask = np.abs(lasso.coef_) > 1e-6\n",
    "selectedIndices = candidateIndices[selectedMask]\n",
    "\n",
    "XReduced = X[:, selectedIndices]\n",
    "dXdtReduced = np.gradient(XReduced, dt, axis=0)\n",
    "\n",
    "# Save the reduced dataset\n",
    "dsReduced = xr.Dataset(\n",
    "    {\n",
    "        \"XReduced\": ((\"time\", \"space\"), XReduced),\n",
    "        \"dXdtReduced\": ((\"time\", \"space\"), dXdtReduced),\n",
    "    },\n",
    "    coords={\n",
    "        \"time\": ds[\"time\"],\n",
    "        \"space\": selectedIndices,\n",
    "    },\n",
    ")\n",
    "\n",
    "dsReduced.to_netcdf(\"data/processed/sstReducedState2COPERNICUS20102019.nc\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (flake)",
   "language": "python",
   "name": "flake-py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
